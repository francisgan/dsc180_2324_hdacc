{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.profiler\n",
    "import logging\n",
    "\n",
    "#%pip install (package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chakra pytorch ET to Chakra ET code (not used for now)\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Dict\n",
    "\n",
    "from chakra.third_party.utils.protolib import encodeMessage as encode_message\n",
    "from chakra.et_def.et_def_pb2 import (\n",
    "    GlobalMetadata,\n",
    "    Node as ChakraNode,\n",
    "    AttributeProto as ChakraAttr,\n",
    "    INVALID_NODE,\n",
    "    COMP_NODE,\n",
    "    COMM_COLL_NODE,\n",
    "    #BOOL,\n",
    "    #FLOAT,\n",
    "    #UINT,\n",
    "    #INT,\n",
    "    #STRING,\n",
    "    #BOOLS,\n",
    "    #FLOATS,\n",
    "    #UINTS,\n",
    "    #INTS,\n",
    "    #STRINGS,\n",
    "    ALL_REDUCE,\n",
    "    ALL_TO_ALL,\n",
    "    ALL_GATHER,\n",
    "    REDUCE_SCATTER,\n",
    "    BROADCAST,\n",
    ")\n",
    "\n",
    "BOOL = bool\n",
    "FLOAT = float\n",
    "UINT = 4\n",
    "INT = int\n",
    "STRING = str\n",
    "BOOLS = 6\n",
    "FLOATS = 7\n",
    "UINTS = 8\n",
    "INTS = 9\n",
    "STRINGS = 10\n",
    "\n",
    "class PyTorch2ChakraConverter:\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_filename: str,\n",
    "            output_filename: str,\n",
    "            num_dims: int,\n",
    "            logger: logging.Logger\n",
    "    ) -> None:\n",
    "        self.input_filename = input_filename\n",
    "        self.output_filename = output_filename\n",
    "        self.num_dims = num_dims\n",
    "        self.logger = logger\n",
    "\n",
    "    @staticmethod\n",
    "    def get_node_type(node: Dict[str, Any]) -> int:\n",
    "        if \"c10d::\" in node[\"name\"]:\n",
    "            return COMM_COLL_NODE\n",
    "        if node[\"op_schema\"] != \"\" or node[\"outputs\"]:\n",
    "            return COMP_NODE\n",
    "        return INVALID_NODE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_attr(\n",
    "            pt_node: Dict[str, Any],\n",
    "            attr_name: str,\n",
    "            attr_type: int\n",
    "    ) -> ChakraAttr:\n",
    "        attr = ChakraAttr(name=attr_name, type=attr_type)\n",
    "\n",
    "        if attr_name in pt_node.keys():\n",
    "            if attr_type == BOOL:\n",
    "                attr.b = pt_node[attr_name]\n",
    "            elif attr_type == FLOAT:\n",
    "                attr.f = pt_node[attr_name]\n",
    "            elif attr_type == UINT:\n",
    "                attr.u = pt_node[attr_name]\n",
    "            elif attr_type == INT:\n",
    "                attr.i = pt_node[attr_name]\n",
    "            elif attr_type == STRING:\n",
    "                attr.s = pt_node[attr_name]\n",
    "            elif attr_type == BOOLS:\n",
    "                attr.bools = pt_node[attr_name]\n",
    "            elif attr_type == FLOATS:\n",
    "                attr.floats = pt_node[attr_name]\n",
    "            elif attr_type == UINTS:\n",
    "                attr.uints = pt_node[attr_name]\n",
    "            elif attr_type == INTS:\n",
    "                attr.ints = pt_node[attr_name]\n",
    "            elif attr_type == STRINGS:\n",
    "                attr.strings = pt_node[attr_name]\n",
    "\n",
    "        return attr\n",
    "\n",
    "    def detect_type(self, node: Dict[str, Any]) -> str:\n",
    "        if node[\"op_schema\"] or node[\"outputs\"]:\n",
    "            return 'operator'\n",
    "        else:\n",
    "            return 'label'\n",
    "\n",
    "    def get_comm_type(self, node: Dict[str, Any]) -> int:\n",
    "        if node[\"name\"] == \"nccl:all_reduce\":\n",
    "            return ALL_REDUCE\n",
    "        elif node[\"name\"] == \"nccl:all_to_all\":\n",
    "            return ALL_TO_ALL\n",
    "        elif (node[\"name\"] == \"nccl:all_gather\")\\\n",
    "            or (node[\"name\"] == \"nccl:_all_gather_base\"):\n",
    "            return ALL_GATHER\n",
    "        elif (node[\"name\"] == \"nccl:reduce_scatter\")\\\n",
    "            or (node[\"name\"] == \"nccl:_reduce_scatter_base\"):\n",
    "            return REDUCE_SCATTER\n",
    "        elif node[\"name\"] == \"nccl:broadcast\":\n",
    "            return BROADCAST\n",
    "        else:\n",
    "            node_name = node[\"name\"]\n",
    "            raise ValueError(f\"{node_name} is not supported\")\n",
    "        return INVALID_COMM\n",
    "\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    # https://github.com/pytorch/pytorch/blob/master/c10/util/Half.h\n",
    "    def get_data_type_size(self, data_type: str) -> int:\n",
    "        data_type_size_dict = {\n",
    "                \"Tensor(float32)\": 4,\n",
    "                \"Tensor(float)\": 4,\n",
    "                \"Tensor(float64)\": 8,\n",
    "                \"Tensor(double)\": 8,\n",
    "                \"Tensor(float16)\": 2,\n",
    "                \"Tensor(half)\": 2,\n",
    "                \"Tensor(bfloat16)\": 2,\n",
    "                \"Tensor(complex64)\": 8,\n",
    "                \"Tensor(complex128)\": 16,\n",
    "                \"Tensor(uint8)\": 1,\n",
    "                \"Tensor(int8)\": 1,\n",
    "                \"Tensor(int16)\": 2,\n",
    "                \"Tensor(short)\": 2,\n",
    "                \"Tensor(int32)\": 4,\n",
    "                \"Tensor(int)\": 4,\n",
    "                \"Tensor(int64)\": 8,\n",
    "                \"Tensor(long)\": 8,\n",
    "                \"Tensor(c10::Half)\": 2,\n",
    "                \"Tensor(unsigned char)\": 1,\n",
    "                \"Tensor(long int)\": 8,\n",
    "        }\n",
    "        try:\n",
    "            data_type_size = data_type_size_dict[data_type]\n",
    "            return data_type_size\n",
    "        except:\n",
    "            raise ValueError(f\"{data_type} is unsupported\")\n",
    "\n",
    "    def get_comm_size(self, node: Dict[str, Any]) -> int:\n",
    "        comm_size = 1\n",
    "        for input_types in node[\"input_types\"]:\n",
    "            comm_size *= self.get_data_type_size(input_types)\n",
    "        for input_shape_outer in node[\"input_shapes\"]:\n",
    "            for input_shape_inner in input_shape_outer:\n",
    "                comm_size = comm_size * input_shape_inner\n",
    "        return comm_size\n",
    "\n",
    "    def dfs(\n",
    "            self,\n",
    "            node: Dict[str, Any],\n",
    "            pytorch_et_data: Dict[str, Any],\n",
    "            pt_node_dict: Dict[int, Dict[str, Any]]\n",
    "    ) -> None:\n",
    "        if self.detect_type(node) == 'operator':\n",
    "            pt_node_dict[node['id']] = node\n",
    "        else:\n",
    "            for pt_node in pytorch_et_data[\"nodes\"]:\n",
    "                if pt_node['parent'] == node['id']:\n",
    "                    self.dfs(pt_node, pytorch_et_data, pt_node_dict)\n",
    "\n",
    "    def convert(self) -> None:\n",
    "        pt_node_dict = {}\n",
    "        ck_node_dict = {}\n",
    "        record_param_comms_pt_node_dict = {}\n",
    "        nccl_pt_node_dict = {}\n",
    "        input_storage_id_node_id_dict = {}\n",
    "        input_tensor_id_node_id_dict = {}\n",
    "        output_storage_id_node_id_dict = {}\n",
    "        output_tensor_id_node_id_dict = {}\n",
    "\n",
    "        with open(self.input_filename, \"r\") as pytorch_et, \\\n",
    "                open(self.output_filename, \"wb\") as chakra_et:\n",
    "            pytorch_et_data = json.load(pytorch_et)\n",
    "\n",
    "            md = GlobalMetadata(\n",
    "              attribute=[\n",
    "                ChakraAttr(name=\"schema\", type=STRING, s=pytorch_et_data[\"schema\"]),\n",
    "                ChakraAttr(name=\"pid\", type=UINT, u=pytorch_et_data[\"pid\"]),\n",
    "                ChakraAttr(name=\"time\", type=STRING, s=pytorch_et_data[\"time\"]),\n",
    "                ChakraAttr(name=\"start_ts\", type=UINT, u=pytorch_et_data[\"start_ts\"]),\n",
    "                ChakraAttr(name=\"finish_ts\", type=UINT, u=pytorch_et_data[\"finish_ts\"])\n",
    "              ]\n",
    "            )\n",
    "            encode_message(chakra_et, md)\n",
    "\n",
    "            self.dfs(pytorch_et_data[\"nodes\"][0], pytorch_et_data, pt_node_dict)\n",
    "\n",
    "            self.logger.info(\"Identify communication nodes\")\n",
    "            for pt_node in pytorch_et_data[\"nodes\"]:\n",
    "                if \"record_param_comms\" in pt_node[\"name\"]:\n",
    "                    record_param_comms_pt_node_dict.update({pt_node[\"parent\"]: pt_node})\n",
    "                if \"nccl:\" in pt_node[\"name\"]:\n",
    "                    nccl_pt_node_dict.update({pt_node[\"parent\"]: pt_node})\n",
    "\n",
    "            self.logger.info(\"Convert PyTorch nodes to Chakra nodes\")\n",
    "            for pt_node_id, pt_node in pt_node_dict.items():\n",
    "                for i in pt_node[\"inputs\"]:\n",
    "                    if isinstance(i, list) and len(i) == 6:\n",
    "                        tensor_id = i[0]\n",
    "                        storage_id = i[1]\n",
    "                        if storage_id > 0:\n",
    "                            input_storage_id_node_id_dict.setdefault(storage_id, []).append(pt_node[\"id\"])\n",
    "                        else:\n",
    "                            input_tensor_id_node_id_dict.setdefault(tensor_id, []).append(pt_node[\"id\"])\n",
    "                for o in pt_node[\"outputs\"]:\n",
    "                    if isinstance(o, list) and len(o) == 6:\n",
    "                        tensor_id = o[0]\n",
    "                        storage_id = o[1]\n",
    "                        if storage_id > 0:\n",
    "                            output_storage_id_node_id_dict.setdefault(storage_id, []).append(pt_node[\"id\"])\n",
    "                        else:\n",
    "                            output_tensor_id_node_id_dict.setdefault(tensor_id, []).append(pt_node[\"id\"])\n",
    "\n",
    "                ck_node = ChakraNode()\n",
    "                ck_node.id = pt_node[\"id\"]\n",
    "                ck_node.name = pt_node[\"name\"]\n",
    "                ck_node.type = self.get_node_type(pt_node)\n",
    "                ck_node.inputs = str(pt_node[\"inputs\"])\n",
    "                ck_node.input_shapes = str(pt_node[\"input_shapes\"])\n",
    "                ck_node.input_types = str(pt_node[\"input_types\"])\n",
    "                ck_node.outputs = str(pt_node[\"outputs\"])\n",
    "                ck_node.output_shapes = str(pt_node[\"output_shapes\"])\n",
    "                ck_node.output_types = str(pt_node[\"output_types\"])\n",
    "\n",
    "                attrs = [(\"fw_parent\", UINT), (\"fw_tid\", UINT), (\"op_schema\", STRING),\n",
    "                        (\"parent\", UINT), (\"seq_id\", INT), (\"rf_id\", UINT), (\"scope\", UINT), (\"tid\", UINT)]\n",
    "                for attr_name, attr_type in attrs:\n",
    "                    attr = self.get_attr(pt_node, attr_name, attr_type)\n",
    "                    ck_node.attribute.append(attr)\n",
    "\n",
    "                # Convert compute nodes\n",
    "                if ck_node.type == COMP_NODE:\n",
    "                    attr = ChakraAttr(name=\"runtime\", type=INT)\n",
    "                    if \"dur\" in pt_node.keys():\n",
    "                        attr.i = pt_node[\"dur\"]\n",
    "                    else:\n",
    "                        attr.i = 0\n",
    "                    ck_node.attribute.append(attr)\n",
    "\n",
    "                # Convert collective communication nodes\n",
    "                elif ck_node.type == COMM_COLL_NODE:\n",
    "                    if ck_node.id in record_param_comms_pt_node_dict.keys():\n",
    "                        record_param_comms_pt_node = record_param_comms_pt_node_dict[ck_node.id]\n",
    "                        nccl_pt_node = nccl_pt_node_dict[record_param_comms_pt_node[\"id\"]]\n",
    "                    else:\n",
    "                        nccl_pt_node = nccl_pt_node_dict[ck_node.id]\n",
    "\n",
    "                    attr = ChakraAttr(name=\"comm_type\", type=INT)\n",
    "                    attr.i = self.get_comm_type(nccl_pt_node)\n",
    "                    ck_node.attribute.append(attr)\n",
    "\n",
    "                    attr = ChakraAttr(name=\"comm_size\", type=INT)\n",
    "                    attr.i = self.get_comm_size(nccl_pt_node)\n",
    "                    ck_node.attribute.append(attr)\n",
    "\n",
    "                    attr = ChakraAttr(name=\"involved_dim\", type=BOOLS)\n",
    "                    for _ in range(self.num_dims):\n",
    "                        attr.bools.append(True)\n",
    "                    ck_node.attribute.append(attr)\n",
    "\n",
    "                ck_node_dict[ck_node.id] = ck_node\n",
    "\n",
    "            self.logger.info(\"Encode data dependency with storage IDs\")\n",
    "            for input_storage_id, child_node_ids in input_storage_id_node_id_dict.items():\n",
    "                if input_storage_id in output_storage_id_node_id_dict:\n",
    "                    parent_node_ids = output_storage_id_node_id_dict[input_storage_id]\n",
    "                    for child_node_id in child_node_ids:\n",
    "                        for parent_node_id in parent_node_ids:\n",
    "                            child_node = ck_node_dict[child_node_id]\n",
    "                            if (parent_node_id not in child_node.parent)\\\n",
    "                            and child_node.id != parent_node_id:\n",
    "                                child_node.parent.append(parent_node_id)\n",
    "\n",
    "                                # remove cycles\n",
    "                                parent_node = ck_node_dict[parent_node_id]\n",
    "                                if (parent_node_id in child_node.parent) and\\\n",
    "                                   (child_node_id in parent_node.parent):\n",
    "                                   if child_node_id < parent_node_id:\n",
    "                                       child_node.parent.remove(parent_node_id)\n",
    "                                   else:\n",
    "                                       parent_node.parent.remove(child_node_id)\n",
    "\n",
    "            self.logger.info(\"Encode data dependency with tensor IDs\")\n",
    "            for input_tensor_id, child_node_ids in input_tensor_id_node_id_dict.items():\n",
    "                if input_tensor_id in output_tensor_id_node_id_dict:\n",
    "                    parent_node_ids = output_tensor_id_node_id_dict[input_tensor_id]\n",
    "                    for child_node_id in child_node_ids:\n",
    "                        for parent_node_id in parent_node_ids:\n",
    "                            child_node = ck_node_dict[child_node_id]\n",
    "                            if (parent_node_id not in child_node.parent)\\\n",
    "                            and child_node.id != parent_node_id:\n",
    "                                child_node.parent.append(parent_node_id)\n",
    "\n",
    "                                # remove cycles\n",
    "                                parent_node = ck_node_dict[parent_node_id]\n",
    "                                if (parent_node_id in child_node.parent) and\\\n",
    "                                   (child_node_id in parent_node.parent):\n",
    "                                   if child_node_id < parent_node_id:\n",
    "                                       child_node.parent.remove(parent_node_id)\n",
    "                                   else:\n",
    "                                       parent_node.parent.remove(child_node_id)\n",
    "\n",
    "            self.logger.info(\"Write Chakra traces\")\n",
    "            for ck_node_id in sorted(ck_node_dict.keys()):\n",
    "                ck_node = ck_node_dict[ck_node_id]\n",
    "                encode_message(chakra_et, ck_node)\n",
    "\n",
    "        self.logger.info(\"All Chakra nodes are written to the output file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAR Model Code\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torchvision\n",
    "from typing import Optional, Union\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_input: int,\n",
    "                 d_model: int,\n",
    "                 d_output: int,\n",
    "                 d_text: int,\n",
    "                 seq_len: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Conv1d(in_channels=d_input, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(d_model)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(d_model)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(d_model*seq_len, d_output)\n",
    "        self.text = nn.Linear(d_model*seq_len, d_text)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b,t,c = x.size()\n",
    "\n",
    "        out = self.layer1(x.permute(0,2,1))\n",
    "        out = self.act1(self.bn1(out))\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        out = self.act2(self.bn2(out))\n",
    "\n",
    "        logits = self.fc(out.reshape(b,-1))\n",
    "        text = self.text(out.reshape(b,-1))\n",
    "\n",
    "        return logits, out.reshape(b,-1), text\n",
    "\n",
    "class CNN_two_heads(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_input: int,\n",
    "                 d_model: int,\n",
    "                 d_class: int,\n",
    "                 d_token: int,\n",
    "                 d_text: int,\n",
    "                 seq_len: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Conv1d(in_channels=d_input, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(d_model)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(d_model)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model*seq_len, d_class)\n",
    "        self.fc2 = nn.Linear(d_model*seq_len, d_token)\n",
    "        self.text = nn.Linear(d_model*seq_len, d_text)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b,t,c = x.size()\n",
    "\n",
    "        out = self.layer1(x.permute(0,2,1))\n",
    "        out = self.act1(self.bn1(out))\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        out = self.act2(self.bn2(out))\n",
    "\n",
    "        logits = self.fc1(out.reshape(b,-1))\n",
    "        tokens = self.fc2(out.reshape(b,-1))\n",
    "\n",
    "        text = self.text(out.reshape(b,-1))\n",
    "\n",
    "        return logits, tokens, out.reshape(b,-1), text\n",
    "\n",
    "class DNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.act3 = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.size()\n",
    "        x = x.reshape(b, -1)\n",
    "        out = self.dropout(self.act1(self.fc1(x)))\n",
    "        out = self.act2(self.fc2(out))\n",
    "        out = self.act3(self.fc3(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class DeepConvLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=128, n_layers=1, n_filters=64, \n",
    "                 n_classes=17, filter_size=5, in_channel=45, drop_prob=0.5):\n",
    "        super(DeepConvLSTM, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_filters = n_filters\n",
    "        self.n_classes = n_classes\n",
    "        self.filter_size = filter_size\n",
    "             \n",
    "        self.conv1 = nn.Conv1d(in_channel, n_filters, filter_size)\n",
    "        self.conv2 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "        self.conv3 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "        self.conv4 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "        \n",
    "        self.lstm1  = nn.LSTM(n_filters, n_hidden, n_layers)\n",
    "        self.lstm2  = nn.LSTM(n_hidden, n_hidden, n_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, x, hidden, batch_size):\n",
    "        \n",
    "        b, t, c = x.size()\n",
    "        #x = x.view(-1, c, t)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x)) #b,c,t\n",
    "        \n",
    "        #x = x.view(x.size(-1), -1, self.n_filters) #t,b,c\n",
    "        x = x.permute(2,0,1)\n",
    "        x, hidden = self.lstm1(x, hidden)\n",
    "        x, hidden = self.lstm2(x, hidden) #t,b,hidden_size\n",
    "        \n",
    "        #x = x.contiguous().view(-1, self.n_hidden) #t*b, hidden_size\n",
    "        x = x.reshape(-1, self.n_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x) #t*b, class_num\n",
    "        #out = x.view(batch_size, -1, self.n_classes)[:,-1,:] #b,t,class_num -> b,class_num\n",
    "        out = x.reshape(-1, batch_size, self.n_classes).permute(1,0,2)[:,-1,:]\n",
    "          \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self,d_input,n_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_input, out_channels=128, kernel_size=8, padding=3)\n",
    "        self.bn1   = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc4   = nn.Linear(128,self.n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b,t,c = x.size()\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x.permute(0,2,1))))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x))) #b,128,t\n",
    "    \n",
    "        x = F.avg_pool1d(x,2) #b,128,t/2\n",
    "        x = torch.mean(x,dim=2) #b,128\n",
    "        x = x.view(-1,128)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.log_softmax(x,1)\n",
    "\n",
    "def correct_sizes(sizes):\n",
    "\tcorrected_sizes = [s if s % 2 != 0 else s - 1 for s in sizes]\n",
    "\treturn corrected_sizes\n",
    "\n",
    "def pass_through(X):\n",
    "\treturn X\n",
    "\n",
    "class Inception(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters, kernel_sizes=[9, 19, 39], bottleneck_channels=32, activation=nn.ReLU(), return_indices=False):\n",
    "\t\t\"\"\"\n",
    "\t\t: param in_channels\t\t\t\tNumber of input channels (input features)\n",
    "\t\t: param n_filters\t\t\t\tNumber of filters per convolution layer => out_channels = 4*n_filters\n",
    "\t\t: param kernel_sizes\t\t\tList of kernel sizes for each convolution.\n",
    "\t\t\t\t\t\t\t\t\t\tEach kernel size must be odd number that meets -> \"kernel_size % 2 !=0\".\n",
    "\t\t\t\t\t\t\t\t\t\tThis is nessesery because of padding size.\n",
    "\t\t\t\t\t\t\t\t\t\tFor correction of kernel_sizes use function \"correct_sizes\". \n",
    "\t\t: param bottleneck_channels\t\tNumber of output channels in bottleneck. \n",
    "\t\t\t\t\t\t\t\t\t\tBottleneck wont be used if nuber of in_channels is equal to 1.\n",
    "\t\t: param activation\t\t\t\tActivation function for output tensor (nn.ReLU()). \n",
    "\t\t: param return_indices\t\t\tIndices are needed only if we want to create decoder with InceptionTranspose with MaxUnpool1d. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Inception, self).__init__()\n",
    "\t\tself.return_indices=return_indices\n",
    "\t\tif in_channels > 1:\n",
    "\t\t\tself.bottleneck = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\tout_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\tself.bottleneck = pass_through\n",
    "\t\t\tbottleneck_channels = 1\n",
    "\n",
    "\t\tself.conv_from_bottleneck_1 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[0], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[0]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_2 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[1], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[1]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.conv_from_bottleneck_3 = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\t\tin_channels=bottleneck_channels, \n",
    "\t\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\t\tkernel_size=kernel_sizes[2], \n",
    "\t\t\t\t\t\t\t\t\t\tstride=1, \n",
    "\t\t\t\t\t\t\t\t\t\tpadding=kernel_sizes[2]//2, \n",
    "\t\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.max_pool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1, return_indices=return_indices)\n",
    "\t\tself.conv_from_maxpool = nn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1, \n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0, \n",
    "\t\t\t\t\t\t\t\t\tbias=False\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\tself.batch_norm = nn.BatchNorm1d(num_features=4*n_filters)\n",
    "\t\tself.activation = activation\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# step 1\n",
    "\t\tZ_bottleneck = self.bottleneck(X)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ_maxpool, indices = self.max_pool(X)\n",
    "\t\telse:\n",
    "\t\t\tZ_maxpool = self.max_pool(X)\n",
    "\t\t# step 2\n",
    "\t\tZ1 = self.conv_from_bottleneck_1(Z_bottleneck)\n",
    "\t\tZ2 = self.conv_from_bottleneck_2(Z_bottleneck)\n",
    "\t\tZ3 = self.conv_from_bottleneck_3(Z_bottleneck)\n",
    "\t\tZ4 = self.conv_from_maxpool(Z_maxpool)\n",
    "\t\t# step 3 \n",
    "\t\tZ = torch.cat([Z1, Z2, Z3, Z4], axis=1)\n",
    "\t\tZ = self.activation(self.batch_norm(Z))\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z, indices\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "\tdef __init__(self, in_channels, n_filters=32, kernel_sizes=[9,19,39], bottleneck_channels=32, use_residual=True, activation=nn.ReLU(), return_indices=False):\n",
    "\t\tsuper(InceptionBlock, self).__init__()\n",
    "\t\tself.use_residual = use_residual\n",
    "\t\tself.return_indices = return_indices\n",
    "\t\tself.activation = activation\n",
    "\t\tself.inception_1 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=in_channels,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_2 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.inception_3 = Inception(\n",
    "\t\t\t\t\t\t\tin_channels=4*n_filters,\n",
    "\t\t\t\t\t\t\tn_filters=n_filters,\n",
    "\t\t\t\t\t\t\tkernel_sizes=kernel_sizes,\n",
    "\t\t\t\t\t\t\tbottleneck_channels=bottleneck_channels,\n",
    "\t\t\t\t\t\t\tactivation=activation,\n",
    "\t\t\t\t\t\t\treturn_indices=return_indices\n",
    "\t\t\t\t\t\t\t)\t\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tself.residual = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.Conv1d(\n",
    "\t\t\t\t\t\t\t\t\tin_channels=in_channels, \n",
    "\t\t\t\t\t\t\t\t\tout_channels=4*n_filters, \n",
    "\t\t\t\t\t\t\t\t\tkernel_size=1,\n",
    "\t\t\t\t\t\t\t\t\tstride=1,\n",
    "\t\t\t\t\t\t\t\t\tpadding=0\n",
    "\t\t\t\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\t\t\tnn.BatchNorm1d(\n",
    "\t\t\t\t\t\t\t\t\tnum_features=4*n_filters\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tif self.return_indices:\n",
    "\t\t\tZ, i1 = self.inception_1(X)\n",
    "\t\t\tZ, i2 = self.inception_2(Z)\n",
    "\t\t\tZ, i3 = self.inception_3(Z)\n",
    "\t\telse:\n",
    "\t\t\tZ = self.inception_1(X)\n",
    "\t\t\tZ = self.inception_2(Z)\n",
    "\t\t\tZ = self.inception_3(Z)\n",
    "\t\tif self.use_residual:\n",
    "\t\t\tZ = Z + self.residual(X)\n",
    "\t\t\tZ = self.activation(Z)\n",
    "\t\tif self.return_indices:\n",
    "\t\t\treturn Z,[i1, i2, i3]\n",
    "\t\telse:\n",
    "\t\t\treturn Z\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class MLSTMfcn(nn.Module):\n",
    "    def __init__(self, *, num_classes, max_seq_len, num_features,\n",
    "                 num_lstm_out=128, num_lstm_layers=1, \n",
    "                 conv1_nf=128, conv2_nf=256, conv3_nf=128,\n",
    "                 lstm_drop_p=0.8, fc_drop_p=0.3):\n",
    "        super(MLSTMfcn, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.num_lstm_out = num_lstm_out\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.conv1_nf = conv1_nf\n",
    "        self.conv2_nf = conv2_nf\n",
    "        self.conv3_nf = conv3_nf\n",
    "\n",
    "        self.lstm_drop_p = lstm_drop_p\n",
    "        self.fc_drop_p = fc_drop_p\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, \n",
    "                            hidden_size=self.num_lstm_out,\n",
    "                            num_layers=self.num_lstm_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.num_features, self.conv1_nf, 8)\n",
    "        self.conv2 = nn.Conv1d(self.conv1_nf, self.conv2_nf, 5)\n",
    "        self.conv3 = nn.Conv1d(self.conv2_nf, self.conv3_nf, 3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv1_nf)\n",
    "        self.bn2 = nn.BatchNorm1d(self.conv2_nf)\n",
    "        self.bn3 = nn.BatchNorm1d(self.conv3_nf)\n",
    "\n",
    "        self.se1 = SELayer(self.conv1_nf)  # ex 128\n",
    "        self.se2 = SELayer(self.conv2_nf)  # ex 256\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstmDrop = nn.Dropout(self.lstm_drop_p)\n",
    "        self.convDrop = nn.Dropout(self.fc_drop_p)\n",
    "\n",
    "        self.fc = nn.Linear(self.conv3_nf+self.num_lstm_out, self.num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ''' input x should be in size [B,T,F], where \n",
    "            B = Batch size\n",
    "            T = Time samples\n",
    "            F = features\n",
    "        '''\n",
    "        #seq_lens = torch.tensor(x.shape[1]).unsqueeze(0).expand(x.shape[0],-1).cuda()\n",
    "        seq_lens = [x.shape[1]] * x.shape[0]\n",
    "        x1 = nn.utils.rnn.pack_padded_sequence(x, seq_lens, \n",
    "                                               batch_first=True, \n",
    "                                               enforce_sorted=False)\n",
    "        x1, (ht,ct) = self.lstm(x1)\n",
    "        x1, _ = nn.utils.rnn.pad_packed_sequence(x1, batch_first=True, \n",
    "                                                 padding_value=0.0)\n",
    "        x1 = x1[:,-1,:]\n",
    "        \n",
    "        x2 = x.transpose(2,1)\n",
    "        x2 = self.convDrop(self.relu(self.bn1(self.conv1(x2))))\n",
    "        x2 = self.se1(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn2(self.conv2(x2))))\n",
    "        x2 = self.se2(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn3(self.conv3(x2))))\n",
    "        x2 = torch.mean(x2,2)\n",
    "        \n",
    "        x_all = torch.cat((x1,x2),dim=1)\n",
    "        x_out = self.fc(x_all)\n",
    "        x_out = F.log_softmax(x_out, dim=1)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "class resConv1dBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, layer_num):\n",
    "        super(resConv1dBlock, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels = in_channels, out_channels = 2 * in_channels, kernel_size = kernel_size, stride = stride, padding = int((kernel_size - 1) / 2) )\n",
    "            for i in range(layer_num)])\n",
    "\n",
    "        self.bn1 = nn.ModuleList([\n",
    "            nn.BatchNorm1d(2 * in_channels)\n",
    "            for i in range(layer_num)])\n",
    "\n",
    "        self.conv2 = nn.ModuleList([ \n",
    "            nn.Conv1d(in_channels = 2 * in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride, padding = int((kernel_size - 1) / 2) )\n",
    "            for i in range(layer_num)])\n",
    "\n",
    "        self.bn2 = nn.ModuleList([\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "            for i in range(layer_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.layer_num):\n",
    "            tmp = F.relu(self.bn1[i](self.conv1[i](x)))\n",
    "            x = F.relu(self.bn2[i](self.conv2[i](tmp)) + x)\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_size, input_channel, num_label):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channel, 64, kernel_size = 1, stride = 1)\n",
    "        self.res1 = resConv1dBlock(64, 64, kernel_size = 3, stride = 1, layer_num = 3)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size = 2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size = 1, stride = 1)\n",
    "        self.res2 = resConv1dBlock(128, 128, kernel_size = 3, stride = 1, layer_num = 4)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size = 2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size = 1, stride = 1)\n",
    "        self.res3 = resConv1dBlock(256, 256,  kernel_size = 3, stride = 1, layer_num = 7)\n",
    "        self.pool3 = nn.AvgPool1d(kernel_size = 2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(256, 128, kernel_size = 1, stride = 1)\n",
    "        self.res4 = resConv1dBlock(128, 128, kernel_size = 3, stride = 1, layer_num = 4)\n",
    "        self.pool = nn.AvgPool1d(kernel_size = int(input_size / 8))\n",
    "\n",
    "        self.fc = nn.Linear(128, num_label)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(self.res1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(self.res2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(self.res3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(self.res4(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class MaCNN(nn.Module):\n",
    "    def __init__(self, input_size, input_channel, num_label, sensor_num):\n",
    "        super(MaCNN, self).__init__()\n",
    "        self.in_channel = int(input_channel / sensor_num)\n",
    "        self.start_conv = nn.ModuleList([nn.Conv1d(self.in_channel, 128, kernel_size = 3, stride = 1, padding = 1) for _ in range(sensor_num)])\n",
    "        self.conv1 = nn.ModuleList([nn.Conv1d(128, 128, kernel_size = 3, stride = 1, padding = 1) for _ in range(sensor_num)])\n",
    "        self.pool1 = nn.ModuleList([nn.AvgPool1d(kernel_size = 2) for _ in range(sensor_num)])\n",
    "        self.conv2 = nn.ModuleList([nn.Conv1d(128, 128, kernel_size = 3, stride = 1, padding = 1) for _ in range(sensor_num)])\n",
    "        self.pool2 = nn.ModuleList([nn.AvgPool1d(kernel_size = 2)for _ in range(sensor_num)])\n",
    "        self.conv3 = nn.ModuleList([nn.Conv1d(128, 128, kernel_size = 3, stride = 1, padding = 1) for _ in range(sensor_num)])\n",
    "        self.pool3 = nn.ModuleList([nn.AvgPool1d(kernel_size = 2)for _ in range(sensor_num)])\n",
    "        self.conv4 = nn.ModuleList([nn.Conv1d(128, 128, kernel_size = 3, stride = 1, padding = 1) for _ in range(sensor_num)])\n",
    "        self.pool4 = nn.ModuleList([nn.AvgPool1d(kernel_size = 2)for _ in range(sensor_num)])\n",
    "        self.conv5 = nn.ModuleList([nn.Conv1d(128, 128, kernel_size = 3, stride = 1, padding = 1) for _ in range(sensor_num)])\n",
    "        self.pool5 = nn.ModuleList([nn.AvgPool1d(kernel_size = 2)for _ in range(sensor_num)])\n",
    "\n",
    "\n",
    "        self.end_conv = nn.ModuleList([nn.Conv1d(128, 1, kernel_size = 1, stride = 1) for _ in range(sensor_num)])\n",
    "        self.Linear = nn.Linear(int(input_size / 32) * sensor_num, num_label)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), x.size(1), -1, self.in_channel).transpose(1, 3)\n",
    "        sensor_num = x.size(2)\n",
    "        x = list(x.split(1, 2))\n",
    "        for i in range(sensor_num):\n",
    "            x[i] = F.relu(self.start_conv[i](x[i].squeeze(2)))\n",
    "\n",
    "            x[i] = F.relu(self.pool1[i](self.conv1[i](x[i])))\n",
    "            x[i] = F.relu(self.pool2[i](self.conv2[i](x[i])))\n",
    "            x[i] = F.relu(self.pool3[i](self.conv3[i](x[i])))\n",
    "            x[i] = F.relu(self.pool4[i](self.conv4[i](x[i])))\n",
    "            x[i] = F.relu(self.pool5[i](self.conv5[i](x[i])))\n",
    "            x[i] = F.relu(self.end_conv[i](x[i])).squeeze(1)\n",
    "\n",
    "            x[i] = x[i].view(x[i].size(0), -1)\n",
    "        x = torch.cat(x, dim = -1)\n",
    "        return self.Linear(x)\n",
    "\n",
    "class Encoder2D(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(Encoder2D, self).__init__()\n",
    "\n",
    "        self.resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)  \n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        #for c in list(self.resnet.children())[5:]:\n",
    "        #    for p in c.parameters():\n",
    "        #        p.requires_grad = fine_tune\n",
    "\n",
    "def generate_original_PE(length: int, d_model: int) -> torch.Tensor:\n",
    "    \"\"\"Generate positional encoding as described in original paper.  :class:`torch.Tensor`\n",
    "    Parameters\n",
    "    ----------\n",
    "    length:\n",
    "        Time window length, i.e. K.\n",
    "    d_model:\n",
    "        Dimension of the model vector.\n",
    "    Returns\n",
    "    -------\n",
    "        Tensor of shape (K, d_model).\n",
    "    \"\"\"\n",
    "    PE = torch.zeros((length, d_model))\n",
    "\n",
    "    pos = torch.arange(length).unsqueeze(1)\n",
    "\n",
    "    PE[:, 0::2] = torch.sin(\n",
    "        pos / torch.pow(1000, torch.arange(0, d_model, 2, dtype=torch.float32)/d_model))\n",
    "    PE[:, 1::2] = torch.cos(\n",
    "        pos / torch.pow(1000, torch.arange(1, d_model, 2, dtype=torch.float32)/d_model))\n",
    "\n",
    "    return PE\n",
    "\n",
    "def generate_regular_PE(length: int, d_model: int, period: Optional[int] = 96) -> torch.Tensor:\n",
    "    \"\"\"Generate positional encoding with a given period.\n",
    "    Parameters\n",
    "    ----------\n",
    "    length:\n",
    "        Time window length, i.e. K.\n",
    "    d_model:\n",
    "        Dimension of the model vector.\n",
    "    period:\n",
    "        Size of the pattern to repeat.\n",
    "        Default is 24.\n",
    "    Returns\n",
    "    -------\n",
    "        Tensor of shape (K, d_model).\n",
    "    \"\"\"\n",
    "    PE = torch.zeros((length, d_model))\n",
    "\n",
    "    pos = torch.arange(length, dtype=torch.float32).unsqueeze(1)\n",
    "    PE = torch.sin(pos * 2 * np.pi / period)\n",
    "    PE = PE.repeat((1, d_model))\n",
    "\n",
    "    return PE\n",
    "\n",
    "def generate_time_PE(length: int, d_model: int) -> torch.Tensor:\n",
    "    \"\"\"Generate positional encoding using time directly.  :class:`torch.Tensor`\n",
    "    Parameters\n",
    "    ----------\n",
    "    length:\n",
    "        Time window length, i.e. K.\n",
    "    d_model:\n",
    "        Dimension of the model vector.\n",
    "    Returns\n",
    "    -------\n",
    "        Tensor of shape (K, d_model).\n",
    "    \"\"\"\n",
    "    PE = torch.zeros((length, d_model))\n",
    "\n",
    "    pos = torch.arange(length).unsqueeze(1)\n",
    "    PE[:, :] = pos\n",
    "    return PE\n",
    "\n",
    "def generate_local_map_mask(chunk_size: int,\n",
    "                            attention_size: int,\n",
    "                            mask_future=False,\n",
    "                            device: torch.device = 'cpu') -> torch.BoolTensor:\n",
    "    \"\"\"Compute attention mask as attention_size wide diagonal.\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_size:\n",
    "        Time dimension size.\n",
    "    attention_size:\n",
    "        Number of backward elements to apply attention.\n",
    "    device:\n",
    "        torch device. Default is ``'cpu'``.\n",
    "    Returns\n",
    "    -------\n",
    "        Mask as a boolean tensor.\n",
    "    \"\"\"\n",
    "    local_map = np.empty((chunk_size, chunk_size))\n",
    "    i, j = np.indices(local_map.shape)\n",
    "\n",
    "    if mask_future:\n",
    "        local_map[i, j] = (i - j > attention_size) ^ (j - i > 0)\n",
    "    else:\n",
    "        #local_map[i, j] = (np.abs(i - j) > attention_size) & ((np.abs(j - i) < 90) | (np.abs(j - i) > 102))\n",
    "        local_map[i , j] = np.abs(i - j) > attention_size\n",
    "        #local_map[i , j] = (np.abs(j - i) < 90) | (np.abs(j - i) > 102)\n",
    "\n",
    "    return torch.BoolTensor(local_map).to(device)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed Forward Network block from Attention is All You Need.\n",
    "    Apply two linear transformations to each input, separately but indetically. We\n",
    "    implement them as 1D convolutions. Input and output have a shape (batch_size, d_model).\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_model:\n",
    "        Dimension of input tensor.\n",
    "    d_ff:\n",
    "        Dimension of hidden layer, default is 2048.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: Optional[int] = 2048):\n",
    "        \"\"\"Initialize the PFF block.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self._linear1 = nn.Linear(d_model, d_ff)\n",
    "        self._linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Propagate forward the input through the PFF block.\n",
    "        Apply the first linear transformation, then a relu actvation,\n",
    "        and the second linear transformation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            Input tensor with shape (batch_size, K, d_model).\n",
    "        Returns\n",
    "        -------\n",
    "            Output tensor with shape (batch_size, K, d_model).\n",
    "        \"\"\"\n",
    "        return self._linear2(F.relu(self._linear1(x)))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi Head Attention block from Attention is All You Need.\n",
    "    Given 3 inputs of shape (batch_size, K, d_model), that will be used\n",
    "    to compute query, keys and values, we output a self attention\n",
    "    tensor of shape (batch_size, K, d_model).\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_model:\n",
    "        Dimension of the input vector.\n",
    "    q:\n",
    "        Dimension of all query matrix.\n",
    "    v:\n",
    "        Dimension of all value matrix.\n",
    "    h:\n",
    "        Number of heads.\n",
    "    attention_size:\n",
    "        Number of backward elements to apply attention.\n",
    "        Deactivated if ``None``. Default is ``None``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "                 attention_size: int = None):\n",
    "        \"\"\"Initialize the Multi Head Block.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self._h = h\n",
    "        self._attention_size = attention_size\n",
    "\n",
    "        # Query, keys and value matrices\n",
    "        self._W_q = nn.Linear(d_model, q*self._h)\n",
    "        self._W_k = nn.Linear(d_model, q*self._h)\n",
    "        self._W_v = nn.Linear(d_model, v*self._h)\n",
    "\n",
    "        # Output linear function\n",
    "        self._W_o = nn.Linear(self._h*v, d_model)\n",
    "\n",
    "        # Score placeholder\n",
    "        self._scores = None\n",
    "\n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor,\n",
    "                mask: Optional[str] = None) -> torch.Tensor:\n",
    "        \"\"\"Propagate forward the input through the MHB.\n",
    "        We compute for each head the queries, keys and values matrices,\n",
    "        followed by the Scaled Dot-Product. The result is concatenated\n",
    "        and returned with shape (batch_size, K, d_model).\n",
    "        Parameters\n",
    "        ----------\n",
    "        query:\n",
    "            Input tensor with shape (batch_size, K, d_model) used to compute queries.\n",
    "        key:\n",
    "            Input tensor with shape (batch_size, K, d_model) used to compute keys.\n",
    "        value:\n",
    "            Input tensor with shape (batch_size, K, d_model) used to compute values.\n",
    "        mask:\n",
    "            Mask to apply on scores before computing attention.\n",
    "            One of ``'subsequent'``, None. Default is None.\n",
    "        Returns\n",
    "        -------\n",
    "            Self attention tensor with shape (batch_size, K, d_model).\n",
    "        \"\"\"\n",
    "        K = query.shape[1]\n",
    "\n",
    "        # Compute Q, K and V, concatenate heads on batch dimension\n",
    "        queries = torch.cat(self._W_q(query).chunk(self._h, dim=-1), dim=0)\n",
    "        keys = torch.cat(self._W_k(key).chunk(self._h, dim=-1), dim=0)\n",
    "        values = torch.cat(self._W_v(value).chunk(self._h, dim=-1), dim=0)\n",
    "\n",
    "        # Scaled Dot Product\n",
    "        self._scores = torch.bmm(queries, keys.transpose(1, 2)) / np.sqrt(K)\n",
    "\n",
    "        # Compute local map mask\n",
    "        if self._attention_size is not None:\n",
    "            attention_mask = generate_local_map_mask(K, self._attention_size, mask_future=False, device=self._scores.device)\n",
    "            self._scores = self._scores.masked_fill(attention_mask, float('-inf'))\n",
    "\n",
    "        # Compute future mask\n",
    "        if mask == \"subsequent\":\n",
    "            future_mask = torch.triu(torch.ones((K, K)), diagonal=1).bool()\n",
    "            future_mask = future_mask.to(self._scores.device)\n",
    "            self._scores = self._scores.masked_fill(future_mask, float('-inf'))\n",
    "\n",
    "        # Apply sotfmax\n",
    "        self._scores = F.softmax(self._scores, dim=-1)\n",
    "\n",
    "        attention = torch.bmm(self._scores, values)\n",
    "\n",
    "        # Concatenat the heads\n",
    "        attention_heads = torch.cat(attention.chunk(self._h, dim=0), dim=-1)\n",
    "\n",
    "        # Apply linear transformation W^O\n",
    "        self_attention = self._W_o(attention_heads)\n",
    "\n",
    "        return self_attention\n",
    "\n",
    "    @property\n",
    "    def attention_map(self) -> torch.Tensor:\n",
    "        \"\"\"Attention map after a forward propagation,\n",
    "        variable `score` in the original paper.\n",
    "        \"\"\"\n",
    "        if self._scores is None:\n",
    "            raise RuntimeError(\n",
    "                \"Evaluate the model once to generate attention map\")\n",
    "        return self._scores\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder block from Attention is All You Need.\n",
    "    Apply Multi Head Attention block followed by a Point-wise Feed Forward block.\n",
    "    Residual sum and normalization are applied at each step.\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_model:\n",
    "        Dimension of the input vector.\n",
    "    q:\n",
    "        Dimension of all query matrix.\n",
    "    v:\n",
    "        Dimension of all value matrix.\n",
    "    h:\n",
    "        Number of heads.\n",
    "    attention_size:\n",
    "        Number of backward elements to apply attention.\n",
    "        Deactivated if ``None``. Default is ``None``.\n",
    "    dropout:\n",
    "        Dropout probability after each MHA or PFF block.\n",
    "        Default is ``0.3``.\n",
    "    chunk_mode:\n",
    "        Swict between different MultiHeadAttention blocks.\n",
    "        One of ``'chunk'``, ``'window'`` or ``None``. Default is ``'chunk'``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "                 attention_size: int = None,\n",
    "                 dropout: float = 0.3,\n",
    "                 chunk_mode: str = 'chunk'):\n",
    "        \"\"\"Initialize the Encoder block\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        MHA = MultiHeadAttention\n",
    "\n",
    "        self._selfAttention = MHA(d_model, q, v, h, attention_size=attention_size)\n",
    "        self._feedForward = PositionwiseFeedForward(d_model)\n",
    "\n",
    "        self._layerNorm1 = nn.LayerNorm(d_model)\n",
    "        self._layerNorm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self._batchNorm1 = nn.BatchNorm1d(d_model)\n",
    "        self._batchNorm2 = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self._dopout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Propagate the input through the Encoder block.\n",
    "        Apply the Multi Head Attention block, add residual and normalize.\n",
    "        Apply the Point-wise Feed Forward block, add residual and normalize.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            Input tensor with shape (batch_size, K, d_model).\n",
    "        Returns\n",
    "        -------\n",
    "            Output tensor with shape (batch_size, K, d_model).\n",
    "        \"\"\"\n",
    "        # Self attention\n",
    "        residual = x\n",
    "        x = self._selfAttention(query=x, key=x, value=x)\n",
    "        x = self._dopout(x)\n",
    "        x = self._layerNorm1(x + residual)\n",
    "        #x = self._batchNorm1((x+residual).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "        # Feed forward\n",
    "        residual = x\n",
    "        x = self._feedForward(x)\n",
    "        x = self._dopout(x)\n",
    "        x = self._layerNorm2(x + residual)\n",
    "        #x = self._batchNorm2((x+residual).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def attention_map(self) -> torch.Tensor:\n",
    "        \"\"\"Attention map after a forward propagation,\n",
    "        variable `score` in the original paper.\n",
    "        \"\"\"\n",
    "        return self._selfAttention.attention_map\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer model from Attention is All You Need.\n",
    "    A classic transformer model adapted for sequential data.\n",
    "    Embedding has been replaced with a fully connected layer,\n",
    "    the last layer softmax is now a sigmoid.\n",
    "    Attributes\n",
    "    ----------\n",
    "    layers_encoding: :py:class:`list` of :class:`Encoder.Encoder`\n",
    "        stack of Encoder layers.\n",
    "    layers_decoding: :py:class:`list` of :class:`Decoder.Decoder`\n",
    "        stack of Decoder layers.\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_input:\n",
    "        Model input dimension.\n",
    "    d_model:\n",
    "        Dimension of the input vector.\n",
    "    d_output:\n",
    "        Model output dimension.\n",
    "    q:\n",
    "        Dimension of queries and keys.\n",
    "    v:\n",
    "        Dimension of values.\n",
    "    h:\n",
    "        Number of heads.\n",
    "    N:\n",
    "        Number of encoder and decoder layers to stack.\n",
    "    attention_size:\n",
    "        Number of backward elements to apply attention.\n",
    "        Deactivated if ``None``. Default is ``None``.\n",
    "    dropout:\n",
    "        Dropout probability after each MHA or PFF block.\n",
    "        Default is ``0.3``.\n",
    "    chunk_mode:\n",
    "        Swict between different MultiHeadAttention blocks.\n",
    "        One of ``'chunk'``, ``'window'`` or ``None``. Default is ``'chunk'``.\n",
    "    pe:\n",
    "        Type of positional encoding to add.\n",
    "        Must be one of ``'original'``, ``'regular'`` or ``None``. Default is ``None``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_input: int,\n",
    "                 d_model: int,\n",
    "                 d_output: int,\n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "                 N: int,\n",
    "                 attention_size: int = None,\n",
    "                 dropout: float = 0.3,\n",
    "                 chunk_mode: bool = True,\n",
    "                 pe: str = None):\n",
    "        \"\"\"Create transformer structure from Encoder and Decoder blocks.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self._d_model = d_model\n",
    "\n",
    "        self.layers_encoding = nn.ModuleList([Encoder(d_model,\n",
    "                                                      q,\n",
    "                                                      v,\n",
    "                                                      h,\n",
    "                                                      attention_size=attention_size,\n",
    "                                                      dropout=dropout,\n",
    "                                                      chunk_mode=chunk_mode) for _ in range(N)])\n",
    "        #self.layers_decoding = nn.ModuleList([Decoder(d_model,\n",
    "        #                                              q,\n",
    "        #                                              v,\n",
    "        #                                              h,\n",
    "        #                                              attention_size=attention_size,\n",
    "        #                                              dropout=dropout,\n",
    "        #                                              chunk_mode=chunk_mode) for _ in range(N)])\n",
    "\n",
    "        self._embedding = nn.Linear(d_input, d_model)\n",
    "        self._linear = nn.Linear(d_model, d_output)\n",
    "        self._embed_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        pe_functions = {\n",
    "            'original': generate_original_PE,\n",
    "            'regular': generate_regular_PE,\n",
    "            'time': generate_time_PE,\n",
    "        }\n",
    "\n",
    "        if pe in pe_functions.keys():\n",
    "            self._generate_PE = pe_functions[pe]\n",
    "        elif pe is None:\n",
    "            self._generate_PE = None\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f'PE \"{pe}\" not understood. Must be one of {\", \".join(pe_functions.keys())} or None.')\n",
    "\n",
    "        self.name = 'transformer'\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Propagate input through transformer\n",
    "        Forward input through an embedding module,\n",
    "        the encoder then decoder stacks, and an output module.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x:\n",
    "            :class:`torch.Tensor` of shape (batch_size, K, d_input).\n",
    "        Returns\n",
    "        -------\n",
    "            Output tensor with shape (batch_size, K, d_output).\n",
    "        \"\"\"\n",
    "        K = x.shape[1]\n",
    "\n",
    "        # Embeddin module\n",
    "        encoding = self._embedding(x)\n",
    "\n",
    "        # Add position encoding\n",
    "        if self._generate_PE is not None:\n",
    "            positional_encoding = self._generate_PE(K, self._d_model)\n",
    "            #positional_encoding = self._embed_linear(positional_encoding.cuda())\n",
    "            positional_encoding = positional_encoding.to(encoding.device)\n",
    "            encoding.add_(positional_encoding)\n",
    "\n",
    "        # Encoding stack\n",
    "        for layer in self.layers_encoding:\n",
    "            encoding = layer(encoding)\n",
    "\n",
    "        # Decoding stack\n",
    "        decoding = encoding\n",
    "        '''\n",
    "        # Add position encoding\n",
    "        if self._generate_PE is not None:\n",
    "            positional_encoding = self._generate_PE(K, self._d_model)\n",
    "            positional_encoding = positional_encoding.to(decoding.device)\n",
    "            decoding.add_(positional_encoding)\n",
    "\n",
    "        for layer in self.layers_decoding:\n",
    "            decoding = layer(decoding, encoding)\n",
    "        '''\n",
    "        # Output module\n",
    "        output = self._linear(decoding)\n",
    "        #output = torch.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_input: int,\n",
    "                 d_model: int,\n",
    "                 d_output: int,\n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "                 N: int,\n",
    "                 seq_len: int, \n",
    "                 d_text: int,\n",
    "                 attention_size: int = None,\n",
    "                 dropout: float = 0.3,\n",
    "                 chunk_mode: bool = True,\n",
    "                 pe: str = None):\n",
    "        \"\"\"Create transformer structure from Encoder and Decoder blocks.\"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(d_input, d_model, d_input, q, v, h, N, attention_size, dropout, chunk_mode, pe)\n",
    "        self.classifier = nn.Linear(seq_len * d_input, d_output)\n",
    "        self.text_classifier = nn.Linear(seq_len * d_input, d_text)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.transformer(x)\n",
    "        b, t, c = out.size()\n",
    "        pred = self.classifier(out.reshape(b, t * c))\n",
    "        text = self.text_classifier(out.reshape(b, t * c))\n",
    "        return pred, out.reshape(b,-1), text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the example data \n",
    "#Read README and download data \n",
    "path = 'data/subject101.dat' # set this to path of dat file, in this case we use subject101.dat\n",
    "data = []\n",
    "with open(path, 'r') as f:\n",
    "    #transform dat into python list\n",
    "    d = f.readlines()\n",
    "    for i in d:\n",
    "        k = i.rstrip().split(\" \")\n",
    "        data.append([float(i) for i in k]) \n",
    "\n",
    "a = torch.tensor([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup CNN Model\n",
    "features = 54 #number of features in data\n",
    "seq_len = 376417 #number of sequences in data\n",
    "number_of_class = 25\n",
    "model_cnn = CNN(features, 64, number_of_class, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<ViewBackward0>),\n",
       " tensor([[nan]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run model\n",
    "model_cnn.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "         nan]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup DNN Model and run it\n",
    "model_dnn = DNN(20326518, 25)\n",
    "model_dnn.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::mkldnn_convolution        46.86%     187.642ms        46.90%     187.783ms      93.891ms     183.80 Mb           0 b             2  \n",
      "                     aten::addmm        22.45%      89.898ms        22.46%      89.912ms      44.956ms         104 b         104 b             2  \n",
      "                     aten::copy_        11.74%      47.002ms        11.74%      47.002ms      15.667ms           0 b           0 b             3  \n",
      "         aten::native_batch_norm        10.76%      43.062ms        10.96%      43.873ms      21.936ms     183.80 Mb      -1.50 Kb             2  \n",
      "                 aten::clamp_min         7.37%      29.510ms         7.37%      29.510ms      14.755ms     183.80 Mb     183.80 Mb             2  \n",
      "                     aten::empty         0.30%       1.193ms         0.30%       1.193ms      56.810us     445.14 Mb     445.14 Mb            21  \n",
      "                    aten::conv1d         0.11%     453.000us        58.96%     236.061ms     118.031ms     183.80 Mb           0 b             2  \n",
      "                     aten::clone         0.08%     318.000us        11.92%      47.719ms      47.719ms      77.54 Mb           0 b             1  \n",
      "                   aten::permute         0.07%     294.000us         0.07%     297.000us     297.000us           0 b           0 b             1  \n",
      "                         aten::t         0.05%     213.000us         0.06%     222.000us     111.000us           0 b           0 b             2  \n",
      "    aten::_batch_norm_impl_index         0.05%     200.000us        11.01%      44.077ms      22.038ms     183.80 Mb           0 b             2  \n",
      "                aten::batch_norm         0.05%     185.000us        11.05%      44.262ms      22.131ms     183.80 Mb           0 b             2  \n",
      "                aten::empty_like         0.04%     160.000us         0.30%       1.205ms     401.667us     261.34 Mb           0 b             3  \n",
      "                      aten::relu         0.01%      48.000us         7.38%      29.558ms      14.779ms     183.80 Mb           0 b             2  \n",
      "              aten::_convolution         0.01%      44.000us        58.84%     235.571ms     117.785ms     183.80 Mb     -77.54 Mb             2  \n",
      "                      aten::view         0.01%      38.000us         0.01%      38.000us      12.667us           0 b           0 b             3  \n",
      "               aten::convolution         0.01%      37.000us        58.84%     235.608ms     117.804ms     183.80 Mb           0 b             2  \n",
      "                      aten::add_         0.01%      22.000us         0.01%      22.000us      11.000us           0 b           0 b             2  \n",
      "                aten::as_strided         0.00%      15.000us         0.00%      15.000us       1.364us           0 b           0 b            11  \n",
      "                    aten::linear         0.00%      13.000us        22.51%      90.147ms      45.074ms         104 b           0 b             2  \n",
      "               aten::as_strided_         0.00%      11.000us         0.00%      11.000us       5.500us           0 b           0 b             2  \n",
      "                   aten::squeeze         0.00%       7.000us         0.00%      10.000us       5.000us           0 b           0 b             2  \n",
      "                 aten::unsqueeze         0.00%       6.000us         0.00%      12.000us       3.000us           0 b           0 b             4  \n",
      "                 aten::transpose         0.00%       6.000us         0.00%       9.000us       4.500us           0 b           0 b             2  \n",
      "                   aten::reshape         0.00%       4.000us         0.01%      42.000us      14.000us           0 b           0 b             3  \n",
      "                aten::contiguous         0.00%       3.000us        11.92%      47.722ms      47.722ms      77.54 Mb           0 b             1  \n",
      "                   aten::resize_         0.00%       3.000us         0.00%       3.000us       1.500us           0 b           0 b             2  \n",
      "                    aten::expand         0.00%       2.000us         0.00%       2.000us       1.000us           0 b           0 b             2  \n",
      "                        [memory]         0.00%       0.000us         0.00%       0.000us       0.000us    -183.80 Mb    -183.80 Mb             6  \n",
      "              aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us           0 b           0 b             4  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 400.389ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get Execution Trace of CNN model and export pytorch ET as json\n",
    "with torch.profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
    "    output = model_cnn(a)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "prof.export_chrome_trace(\"cnn_trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m converter \u001b[39m=\u001b[39m PyTorch2ChakraConverter(\u001b[39m\"\u001b[39m\u001b[39mcnn_trace.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m, logger)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m converter\u001b[39m.\u001b[39;49mconvert()\n",
      "\u001b[1;32m/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_filename, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m pytorch_et, \\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m         \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_filename, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m chakra_et:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m     pytorch_et_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(pytorch_et)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m     md \u001b[39m=\u001b[39m GlobalMetadata(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m       attribute\u001b[39m=\u001b[39m[\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m         ChakraAttr(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mschema\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mSTRING, s\u001b[39m=\u001b[39mpytorch_et_data[\u001b[39m\"\u001b[39;49m\u001b[39mschema\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m         ChakraAttr(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mUINT, u\u001b[39m=\u001b[39mpytorch_et_data[\u001b[39m\"\u001b[39m\u001b[39mpid\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m         ChakraAttr(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mSTRING, s\u001b[39m=\u001b[39mpytorch_et_data[\u001b[39m\"\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m         ChakraAttr(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstart_ts\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mUINT, u\u001b[39m=\u001b[39mpytorch_et_data[\u001b[39m\"\u001b[39m\u001b[39mstart_ts\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m         ChakraAttr(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfinish_ts\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mUINT, u\u001b[39m=\u001b[39mpytorch_et_data[\u001b[39m\"\u001b[39m\u001b[39mfinish_ts\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m       ]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m     encode_message(chakra_et, md)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/gl/Desktop/dsc180_2324_hdacc/test.ipynb#X11sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdfs(pytorch_et_data[\u001b[39m\"\u001b[39m\u001b[39mnodes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m], pytorch_et_data, pt_node_dict)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'schema'"
     ]
    }
   ],
   "source": [
    "#Working on it ...\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "converter = PyTorch2ChakraConverter(\"cnn_trace.json\", '1', 1, logger)\n",
    "converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
